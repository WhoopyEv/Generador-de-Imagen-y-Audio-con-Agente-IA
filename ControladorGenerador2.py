""""
 ESTA ES UNA VERSIÃ“N ANTERIOR DEL CÃ“DIGO, DONDE 
 NO INCORPORABA AGENTES NI TOOLS DE LANGCHAIN.
"""

import json
import re
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage
from GeneradorImagenes import ImagenIA
from GeneradorAudio import GeneradorAudio


class ControladorGeneradorIA:
    def __init__(self, url_llm="http://localhost:1234/v1", modelo_llm="local-model"):
        self.llm = ChatOpenAI(
            model_name=modelo_llm,
            openai_api_base=url_llm,
            openai_api_key="not-needed"
        )
        self.generador_img = ImagenIA()
        self.generador_audio = GeneradorAudio(voz="es-CO-SalomeNeural")

    def clasificar_modo(self, idea: str) -> str:
        mensaje = (
            f"""
            Un usuario ha dado el siguiente texto para generar contenido con IA multimodal.

            Clasifica la intenciÃ³n respondiendo solo con una de estas tres palabras exactas:
            - "imagen" si solo quiere una imagen.
            - "audio" si solo quiere escuchar una narraciÃ³n.
            - "ambos" si desea tanto imagen como narraciÃ³n en audio.

            No expliques tu respuesta. Solo escribe una palabra: imagen, audio o ambos.

            Texto del usuario:
            {idea}
           """
        )
        respuesta = self.llm([HumanMessage(content=mensaje)]).content.strip().lower()
        if respuesta not in ["audio", "imagen", "ambos"]:
            print(f"âš ï¸ Respuesta inesperada: {respuesta}. Se usarÃ¡ 'ambos' por defecto.")
            return "ambos"
        return respuesta

    def generar_prompts(self, idea: str) -> tuple:
        mensaje = f"""
Convierte esta idea en dos salidas separadas en formato JSON, asÃ­:

{{
  "texto_narrativo": "...",
  "prompt_imagen": "..."
}}

- "texto_narrativo": debe ser un pequeÃ±o relato o descripciÃ³n en espaÃ±ol, amigable para leer en voz alta.
- "prompt_imagen": debe ser un prompt visual en inglÃ©s, estilo Stable Diffusion.

Texto: {idea}
"""
        respuesta = self.llm([HumanMessage(content=mensaje)]).content
        print("ðŸ§¾ Respuesta cruda del modelo:\n", respuesta)  # Ãštil para debug

        # Extraer primer bloque JSON vÃ¡lido con expresiÃ³n regular
        coincidencias = re.findall(r'\{[^{}]*"texto_narrativo"[^{}]*"prompt_imagen"[^{}]*\}', respuesta, re.DOTALL)

        if not coincidencias:
            raise ValueError("âŒ No se encontrÃ³ un bloque JSON vÃ¡lido en la respuesta del modelo")

        try:
            datos = json.loads(coincidencias[0])
        except json.JSONDecodeError as e:
            raise ValueError(f"âŒ Error al parsear JSON: {e}\nTexto recibido:\n{coincidencias[0]}")

        return datos["texto_narrativo"], datos["prompt_imagen"]

    def ejecutar(self, idea: str):
        modo = self.clasificar_modo(idea)
        texto, prompt = self.generar_prompts(idea)

        print(f"\nðŸ—£ï¸ Texto narrativo:\n{texto}\n")
        print(f"ðŸ–¼ï¸ Prompt visual:\n{prompt}\n")

        if modo in ["imagen", "ambos"]:
            self.generador_img.generar_imagen(prompt)

        if modo in ["audio", "ambos"]:
            self.generador_audio.ejecutar(texto, "voz_ia.mp3")

    def ejecutar_con_retorno(self, idea: str) -> tuple:
        modo = self.clasificar_modo(idea)
        texto, prompt = self.generar_prompts(idea)

        ruta_img = ruta_audio = None

        if modo in ["imagen", "ambos"]:
            ruta_img = self.generador_img.generar_imagen(prompt)

        if modo in ["audio", "ambos"]:
            ruta_audio = self.generador_audio.ejecutar(texto)

        return modo, texto, prompt, ruta_img, ruta_audio


if __name__ == "__main__":
    controlador = ControladorGeneradorIA()
    idea = input("ðŸ§  Escribe tu idea o descripciÃ³n: ")
    controlador.ejecutar(idea)